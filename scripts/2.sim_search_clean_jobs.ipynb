{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script attempts to clean the jobs data further using regex, as well as an embedding model to perform similarity search to extract parts of a job description which are relevant to ISCO classification i.e. tasks and skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hydraze/anaconda3/envs/es24_final/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/hydraze/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths: Input is output of 1.data_processing.ipynb\n",
    "jobs_input_path = '../output/jobs_ts.csv'\n",
    "jobs_output_path = '../output/jobs_ts_cleaned_simsearch_regex.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "jobs_ts = pd.read_csv(jobs_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentences for each jd, facilitates cleaning downstream\n",
    "jobs_ts['title_desc_ts_clean_sent_tok'] = [nltk.tokenize.sent_tokenize(td) for td in jobs_ts['title_desc_ts_postclean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out sentences which contain patterns unrelated to a job description. These sentences tend to be with respect to PDPA, or are \n",
    "# text belonging to the website which the JDs were scraped from e.g. \"log in here\", or \"forgot password\", which were likely to be buttons \n",
    "# on websites. \n",
    "def is_match(regex, text):\n",
    "    pattern = re.compile(regex)\n",
    "    return pattern.search(text) is not None\n",
    "\n",
    "# Compile keywords for removal\n",
    "regex = r\"personal data|discriminat.*|i agree home|privacy policy|cookie|confirmation link|\\slogin\\s|log in|\\scv\\s|curriculum vitae|recommended browser|job alert|forgot password|t&c|job opportunities|sign up|receive notification|job ads|click here|resume\"\n",
    "\n",
    "all_cleaned_sent_tok_list = [] # To store all rows of data\n",
    "removed_sent_list = [] # If want to perform analysis on what is being removed\n",
    "for sent_tokenised in jobs_ts['title_desc_ts_clean_sent_tok']:\n",
    "    cleaned_sent_tok_list = [] # List to store cleaned sentences for each row of data\n",
    "    \n",
    "    # For each sentence, keep them if they are not matching the keywords above\n",
    "    for sent in sent_tokenised:\n",
    "        if not is_match(regex, sent) and len(sent) > 2: # Ensure that empty spaces, and single letter/character splits are removed\n",
    "            sent = sent.strip()\n",
    "            cleaned_sent_tok_list.append(sent)\n",
    "        else:\n",
    "            removed_sent_list.append(sent)\n",
    "            \n",
    "    all_cleaned_sent_tok_list.append(cleaned_sent_tok_list)\n",
    "\n",
    "jobs_ts['title_desc_ts_clean_sent_tok_regex'] = all_cleaned_sent_tok_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with the pre-regex tokenised sentence if everything was removed by the regex filtering above\n",
    "# sometimes the JDs go on and on without any full stops and form one long sentence, and the keywords above are part of it. \n",
    "jd_removed_bool = (jobs_ts['title_desc_ts_clean_sent_tok_regex'].apply(len) == 0) # Check which ones are empty lists. If there is at least one sentence which passes the filter, it would have a length of 1\n",
    "jobs_ts.loc[jd_removed_bool, 'title_desc_ts_clean_sent_tok_regex'] = jobs_ts.loc[jd_removed_bool, 'title_desc_ts_clean_sent_tok']\n",
    "\n",
    "jobs_ts['title_desc_ts_postclean_regex'] = jobs_ts['title_desc_ts_clean_sent_tok_regex'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning sentences using a sentence transformer, continuing on from the previously regex-filtered tokenised sentences\n",
    "\n",
    "The core idea is that if a sentence is not similar to the idea of tasks and skills (semantically at least), which is what the ISCO is based on, it should not be part of the JD. Similarity is based on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e020711724834c09bd85e5ffc21dde43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "tqdm.pandas(desc='Sentence searching in progress')\n",
    "\n",
    "batch_size=50\n",
    "similarity_threshold = 0.50 # tested on 42 sampled cases to remove a significant amount of noisy text\n",
    "model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", trust_remote_code=True) # performed reasonably well when looking at the text removed\n",
    "hf_revision = \"5c38ec7c405ec4b44b94cc5a9bb96e735b38267a\" # version number of the bge sentence transformer\n",
    "\n",
    "# this query encapsulates what the ISCO classification is based on, and each sentence in the JD will be compared against this query by calculating\n",
    "# the cosine similarity. To only keep the sentences which have enough cosine similarity with the query.\n",
    "query = \"Job titles, professions, tasks and skills.\" \n",
    "embeddings_query = model.encode(query, device=\"cpu\", batch_size=batch_size, normalize_embeddings=True, convert_to_numpy=True,\n",
    "                                revision=hf_revision\n",
    "                             )\n",
    "\n",
    "# Same structure as filtering using regex above \n",
    "all_cleaned_sent_tok_list = [] \n",
    "all_removed_sent_list = []\n",
    "for idx, sent_tokenised in tqdm(enumerate(jobs_ts['title_desc_ts_clean_sent_tok_regex']), total=len(jobs_ts['title_desc_ts_clean_sent_tok_regex'])):\n",
    "    # convert the sentences into normalised embeddings\n",
    "    embeddings_sent_tok = model.encode(sent_tokenised, device=\"cpu\", batch_size=batch_size, normalize_embeddings=True, convert_to_numpy=True,\n",
    "                                       revision=hf_revision)\n",
    "    similarity =  embeddings_sent_tok @ embeddings_query.T # calculate cosine similarity score between embeddings (embeddings are already normalized)\n",
    "    \n",
    "    # Get only sentences where they are similar enough to the query i.e. the ISCO classification criteria\n",
    "    inclusion_filter_bool = (similarity > similarity_threshold) \n",
    "    cleaned_sent_tok_list = list(np.array(sent_tokenised)[np.where(inclusion_filter_bool)]) \n",
    "    \n",
    "    # Keeping the first sentence i.e. the job title, if it was removed by the similarity search (these tend to be job titles which are more unique e.g. packers, dock workers)\n",
    "    if sent_tokenised[0] not in cleaned_sent_tok_list: \n",
    "        cleaned_sent_tok_list = [sent_tokenised[0]] + cleaned_sent_tok_list\n",
    "    \n",
    "    # Compile all the sentences which passed the similarity check\n",
    "    all_cleaned_sent_tok_list.append(cleaned_sent_tok_list)\n",
    "    \n",
    "    # If want to check for which sentences are being removed\n",
    "    removed_sent_list = list(np.array(sent_tokenised)[np.where(~inclusion_filter_bool)])\n",
    "    all_removed_sent_list.append(removed_sent_list)\n",
    "    \n",
    "jobs_ts['title_desc_ts_clean_sent_tok_regex_simsearch'] = all_cleaned_sent_tok_list\n",
    "\n",
    "# Join all tokenised sentences which passed the similarity check\n",
    "jobs_ts['title_desc_ts_postclean_regex_simsearch'] = jobs_ts['title_desc_ts_clean_sent_tok_regex_simsearch'].apply(' '.join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows had 0 sentences after applying regex + sim search cleaning\n"
     ]
    }
   ],
   "source": [
    "# fill in with the regex-cleaned tokenised sentence if everything was removed. \n",
    "jd_removed_bool = jobs_ts['title_desc_ts_postclean_regex_simsearch'].apply(len) == 0\n",
    "print(sum(jd_removed_bool), 'rows had 0 sentences after applying regex + sim search cleaning')\n",
    "\n",
    "jobs_ts.loc[jd_removed_bool, 'title_desc_ts_postclean_regex_simsearch'] = jobs_ts.loc[jd_removed_bool, 'title_desc_ts_postclean_regex']\n",
    "assert min(jobs_ts['title_desc_ts_postclean_regex_simsearch'].apply(len)) > 0, \"still have rows which have no values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "jobs_ts.to_csv(jobs_output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es24_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
