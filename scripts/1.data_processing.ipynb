{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script does preliminary cleaning of jobs and isco data.\n",
    "\n",
    "Additionally, it performs the language detection and translation of all job description data from original language to english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "jobs_input_path = '../data/wi_dataset.csv' # Original data\n",
    "isco_input_path = '../data/wi_labels.csv' # Original data\n",
    "\n",
    "jobs_output_path = '../output/jobs_ts.csv' # Need to create an outputs folder\n",
    "isco_output_path = '../output/wi_labels_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "jobs = pd.read_csv(jobs_input_path)\n",
    "isco = pd.read_csv(isco_input_path, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing for ISCO code and description dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "isco['description'] = isco['description'].str.lower()\n",
    "\n",
    "# These additional notes may end up saying how another classification would be better, and may confuse the embedding model,\n",
    "# so to remove them. \"notes\" are always after \"some related occupations...\", so remove notes first, just to be sure\n",
    "isco['description'] = (isco['description']\n",
    "                       .str.replace(r'(notes\\n.*)', '', regex=True)\n",
    "                       .str.replace(r'(some related occupations classified elsewhere.*)', '', regex=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing for job description dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning job descriptions\n",
    "jobs['description_clean'] = jobs['description'].fillna('') # Fill missing descriptions\n",
    "\n",
    "# Remove all identified special characters from job description and title before language detection and translation\n",
    "special_characters = r'[$?^+!@#*_â˜…ðŸ˜Šï¿½â„–â†’â†—â„¢â‡§âˆ‚â‡¨âˆ™âˆšâ¬âˆ’â°â””â”‚â– â–ªâ–¬â–¶â–¸â–ºâ–¼â—†â—â—¢â—¥â—¾â˜€â˜›â™€â™¦âš“âš½âœ…âœ†âœ‰âœâœ“âœ”âœ©âœ´âœ¶â–â¤â¯âž âž¡âž¢âž¤âž§âž­âž²âž½â €â¦â«½â¬Ÿâ­Â¤Â¦Â§Â¨Â©ÂªÂ«Â¬Â®Â°Â²Â³Â´ÂµÂ·ÂºÂ»Â¿â€¡â€ž\\ufeff]'\n",
    "jobs['description_clean'] = jobs['description_clean'].str.replace(special_characters, '', regex = True)\n",
    "jobs['title_clean'] =  jobs['title'].str.replace(special_characters, '', regex = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lingua covers all EU languages, hence can use it for detection of languages in this dataset. \n",
    "# Loading languages to be used for detection\n",
    "languages = [Language.BULGARIAN, Language.CROATIAN, Language.CZECH, Language.DANISH, Language.DUTCH, Language.ENGLISH, Language.ESTONIAN,\n",
    "             Language.FINNISH, Language.FRENCH, Language.GERMAN, Language.GREEK, Language.HUNGARIAN, Language.IRISH, Language.ITALIAN,\n",
    "             Language.LATVIAN, Language.LITHUANIAN, Language.POLISH, Language.PORTUGUESE, Language.ROMANIAN, Language.SLOVAK, \n",
    "             Language.SLOVENE, Language.SPANISH, Language.SWEDISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting language of job title and description. \n",
    "jobs['lang_desc'] = [detector.detect_language_of(description) for description in jobs['description_clean']]\n",
    "jobs['lang_desc'] = jobs['lang_desc'].astype(str).str.replace('Language.', '') # So that the output fits the translation step better\n",
    "\n",
    "jobs['lang_title'] = [detector.detect_language_of(title) for title in jobs['title_clean']]\n",
    "jobs['lang_title'] = jobs['lang_title'].astype(str).str.replace('Language.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclude on a final language: Priority for language based on description first since it has more text.\n",
    "# Furthermore, it's not the job title that is being classified, but the job description.\n",
    "# Lastly, sometimes the job titles are in English, but the desciption is in host country's language\n",
    "# Sometimes the language of the description cannot be detected, returning None.\n",
    "jobs.loc[(jobs['lang_desc'] == \"None\"), 'lang_desc'] = None \n",
    "jobs['lang'] = jobs['lang_desc'].combine_first(jobs['lang_title']) # Fill in empty lang_desc with lang_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preprocessing for translation step: tokenise into sentences before translating --> less chances of getting a translation error\n",
    "# from sending too much text to google API.\n",
    "jobs['title_and_desc'] = jobs['title_clean'] + \". \" + jobs['description_clean']\n",
    "jobs['td_sent_tok'] = [nltk.tokenize.sent_tokenize(td) for td in jobs['title_and_desc']] # Should work for most languages\n",
    "jobs['lang'] = jobs['lang'].str.replace('SLOVENE', 'SLOVENIAN').str.lower() # To match what is required of translator package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation from original JD language to English. This phase took around 3-6 hrs\n",
    "tqdm.pandas(desc='translation in progress')\n",
    "\n",
    "jobs_ts = jobs.copy()\n",
    "lang_map = GoogleTranslator().get_supported_languages(as_dict=True) \n",
    "\n",
    "jobs_ts['lang_sf'] = jobs_ts['lang'].map(lang_map) # So can get the language codes used by google translate, which is different from that used by lingua\n",
    "\n",
    "def gs_translate_desc(row):\n",
    "    # No need to translate if the detected language is english\n",
    "    if (row['lang_sf'] == \"en\"):\n",
    "        return row['td_sent_tok']\n",
    "    else:\n",
    "        # Send tokenised sentences to google translate in batches. \n",
    "        # Input JD language is as per determined by Lingua package. Output should always be english\n",
    "        try:\n",
    "            return GoogleTranslator(source=row['lang_sf'], target='en').translate_batch(row['td_sent_tok'])\n",
    "        \n",
    "        # Possibly translation failed due to poor internet connection (either fail to send, or fail to receive from Google translate API\n",
    "        except Exception as e:\n",
    "            print(\"Failed to translate. Error: \", e)\n",
    "            return ['Failed to translate']\n",
    "    \n",
    "jobs_ts['title_desc_ts'] = jobs_ts.progress_apply(gs_translate_desc, axis=1) # Progress apply is for tqdm, to track progress of translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate for any translations that didn't go through. Occurs due to unstable internet connection, over the 3-6 hrs that the\n",
    "# previous code snippets run for. Keep running this part until all transla\n",
    "failed_to_ts_bool = jobs_ts['title_desc_ts'].isin([['Failed to translate']])\n",
    "while sum(failed_to_ts_bool) > 0:\n",
    "    # Apply the translation function to only those rows which failed previously\n",
    "    jobs_ts.loc[failed_to_ts_bool, \"title_desc_ts\"] = jobs_ts.loc[failed_to_ts_bool, :].progress_apply(gs_translate_desc, axis=1) \n",
    "    failed_to_ts_bool = jobs_ts['title_desc_ts'].isin([['Failed to translate']]) # Redetermine the failures\n",
    "    print(\"Number of failed translations: \", sum(failed_to_ts_bool))\n",
    "    time.sleep(5) # Wait 5 seconds before trying again\n",
    "    \n",
    "print(\"No failed translations, please proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the translated sentences into one JD, \n",
    "# except the \"None\"s (those which were not able to be translated, likely a sequence of symbols e.g. ----------------)\n",
    "def remove_none_concat(title_desc_ts):\n",
    "    return ' '.join([sent for sent in title_desc_ts if sent != None])\n",
    "\n",
    "jobs_ts['title_desc_ts_clean'] = jobs_ts['title_desc_ts'].apply(remove_none_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further cleaning remove non-ASCII since main language is English now\n",
    "jobs_ts['title_desc_ts_postclean'] = [re.sub(r'[^\\x00-\\x7f]', '', x).replace(\" \", \" \") for x in jobs_ts['title_desc_ts_clean']]\n",
    "jobs_ts['title_desc_ts_postclean'] = jobs_ts['title_desc_ts_postclean'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "jobs_ts.drop(columns=['title_and_desc', 'td_sent_tok',\n",
    "                      'lang_sf', 'title_desc_ts']).to_csv(jobs_output_path, index=False)\n",
    "\n",
    "isco.to_csv(isco_output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es24_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
